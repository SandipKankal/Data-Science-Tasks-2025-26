# -*- coding: utf-8 -*-
"""DS_Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ticqG7iRjVhm5y_Aac7LQ-H22aLFaKT4
"""

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve
import joblib
import sys
import os

class SuppressPrints:
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout


with SuppressPrints():
    pass  # Already imported above
    np.random.seed(42)
    n_samples = 1000

    data = pd.DataFrame({
        'Gender': np.random.choice(['Male', 'Female'], n_samples),
        'Married': np.random.choice(['Yes', 'No'], n_samples),
        'Dependents': np.random.choice(['0', '1', '2', '3+'], n_samples),
        'Education': np.random.choice(['Graduate', 'Not Graduate'], n_samples),
        'Self_Employed': np.random.choice(['Yes', 'No'], n_samples, p=[0.15, 0.85]),
        'ApplicantIncome': np.random.randint(1000, 10000, n_samples),
        'CoapplicantIncome': np.random.randint(0, 5000, n_samples),
        'LoanAmount': np.random.randint(50, 500, n_samples),
        'Loan_Amount_Term': np.random.choice([360, 180, 240, 120], n_samples, p=[0.7, 0.15, 0.1, 0.05]),
        'Credit_History': np.random.choice([1.0, 0.0], n_samples, p=[0.85, 0.15]),
        'Property_Area': np.random.choice(['Urban', 'Semiurban', 'Rural'], n_samples)
    })

    data['Loan_Status'] = 'N'
    data.loc[(data['Credit_History'] == 1.0) & (data['ApplicantIncome'] > 3000), 'Loan_Status'] = 'Y'
    data.loc[(data['Credit_History'] == 0.0) & (data['ApplicantIncome'] > 7000), 'Loan_Status'] = 'Y'
    mask = np.random.rand(n_samples) < 0.15
    data.loc[mask, 'Loan_Status'] = np.random.choice(['Y', 'N'], mask.sum())

    data.loc[np.random.choice(data.index, 50), 'Gender'] = np.nan
    data.loc[np.random.choice(data.index, 30), 'Married'] = np.nan
    data.loc[np.random.choice(data.index, 40), 'LoanAmount'] = np.nan
    data.loc[np.random.choice(data.index, 20), 'Credit_History'] = np.nan

    plt.ioff()
    fig = plt.figure(figsize=(18, 12))

    plt.close('all')

    df = data.copy()

    # Missing Values
    categorical_cols = ['Gender', 'Married', 'Dependents', 'Self_Employed']
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].mode()[0], inplace=True)

    numerical_cols = ['LoanAmount', 'Credit_History', 'Loan_Amount_Term']
    for col in numerical_cols:
        if df[col].isnull().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

    def detect_and_remove_outliers(df, column):
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_clean = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
        return df_clean, 0

    outlier_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
    for col in outlier_cols:
        df, _ = detect_and_remove_outliers(df, col)

    df['TotalIncome'] = df['ApplicantIncome'] + df['CoapplicantIncome']
    df['EMI'] = df['LoanAmount'] / df['Loan_Amount_Term']
    df['Balance_Income'] = df['TotalIncome'] - (df['EMI'] * 1000)

    label_encoders = {}
    categorical_features = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents']
    for col in categorical_features:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le

    le_target = LabelEncoder()
    df['Loan_Status'] = le_target.fit_transform(df['Loan_Status'])
    label_encoders['Loan_Status'] = le_target

    X = df.drop(['Loan_Status'], axis=1)
    y = df['Loan_Status']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5)
    }

    results = {}
    for model_name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred_train = model.predict(X_train_scaled)
        y_pred_test = model.predict(X_test_scaled)
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None

        train_accuracy = accuracy_score(y_train, y_pred_train)
        test_accuracy = accuracy_score(y_test, y_pred_test)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)

        results[model_name] = {
            'model': model,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'cv_scores': cv_scores,
            'y_pred_test': y_pred_test,
            'y_pred_proba': y_pred_proba
        }

    best_model_name = max(results, key=lambda x: results[x]['test_accuracy'])
    best_model = results[best_model_name]['model']

    fig = plt.figure(figsize=(16, 10))
    # ... all evaluation plots
    plt.close('all')

    joblib.dump(best_model, 'best_loan_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')
    joblib.dump(label_encoders, 'label_encoders.pkl')

def predict_loan_approval(new_data_dict):
    model = joblib.load('best_loan_model.pkl')
    scaler = joblib.load('scaler.pkl')
    encoders = joblib.load('label_encoders.pkl')

    new_df = pd.DataFrame([new_data_dict])

    for col in ['Gender', 'Married', 'Dependents', 'Self_Employed']:
        if col in new_df.columns and new_df[col].isnull().any():
            new_df[col].fillna('Yes', inplace=True)

    for col in ['LoanAmount', 'Credit_History', 'Loan_Amount_Term']:
        if col in new_df.columns and new_df[col].isnull().any():
            new_df[col].fillna(new_df[col].median(), inplace=True)

    new_df['TotalIncome'] = new_df['ApplicantIncome'] + new_df['CoapplicantIncome']
    new_df['EMI'] = new_df['LoanAmount'] / new_df['Loan_Amount_Term']
    new_df['Balance_Income'] = new_df['TotalIncome'] - (new_df['EMI'] * 1000)

    for col in ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents']:
        if col in new_df.columns and col in encoders:
            le = encoders[col]
            new_df[col] = le.transform(new_df[col])

    new_scaled = scaler.transform(new_df)
    prediction = model.predict(new_scaled)[0]

    if hasattr(model, 'predict_proba'):
        probability = model.predict_proba(new_scaled)[0]
        prob_rejected = probability[0]
        prob_approved = probability[1]
    else:
        prob_rejected = prob_approved = None

    loan_status = encoders['Loan_Status'].inverse_transform([prediction])[0]
    return loan_status, prob_rejected, prob_approved


# Test Case 1
test_case_1 = {
    'Gender': 'Male', 'Married': 'Yes', 'Dependents': '0', 'Education': 'Graduate',
    'Self_Employed': 'No', 'ApplicantIncome': 5000, 'CoapplicantIncome': 2000,
    'LoanAmount': 150, 'Loan_Amount_Term': 360, 'Credit_History': 1.0, 'Property_Area': 'Urban'
}

print("\n Test Case 1: Strong Applicant")
for key, value in test_case_1.items():
    print(f"  {key:<20}: {value}")

result, prob_no, prob_yes = predict_loan_approval(test_case_1)
print(f"\n Prediction: {'APPROVED ✅' if result == 'Y' else 'REJECTED ❌'}")
if prob_yes is not None:
    print(f"   Probability of Rejection: {prob_no:.2%}")
    print(f"   Probability of Approval: {prob_yes:.2%}")

# Test Case 2
test_case_2 = {
    'Gender': 'Female', 'Married': 'No', 'Dependents': '2', 'Education': 'Not Graduate',
    'Self_Employed': 'Yes', 'ApplicantIncome': 2000, 'CoapplicantIncome': 0,
    'LoanAmount': 300, 'Loan_Amount_Term': 360, 'Credit_History': 0.0, 'Property_Area': 'Rural'
}

print("\n Test Case 2: Weak Applicant")
for key, value in test_case_2.items():
    print(f"  {key:<20}: {value}")

result, prob_no, prob_yes = predict_loan_approval(test_case_2)
print(f"\n Prediction: {'APPROVED ✅' if result == 'Y' else 'REJECTED ❌'}")
if prob_yes is not None:
    print(f"   Probability of Rejection: {prob_no:.2%}")
    print(f"   Probability of Approval: {prob_yes:.2%}")

# Test Case 3
test_case_3 = {
    'Gender': 'Male', 'Married': 'Yes', 'Dependents': '1', 'Education': 'Graduate',
    'Self_Employed': 'No', 'ApplicantIncome': 4000, 'CoapplicantIncome': 1500,
    'LoanAmount': 200, 'Loan_Amount_Term': 360, 'Credit_History': 1.0, 'Property_Area': 'Semiurban'
}

print("\n Test Case 3: Average Applicant")
for key, value in test_case_3.items():
    print(f"  {key:<20}: {value}")

result, prob_no, prob_yes = predict_loan_approval(test_case_3)
print(f"\n Prediction: {'APPROVED ✅' if result == 'Y' else 'REJECTED ❌'}")
if prob_yes is not None:
    print(f"   Probability of Rejection: {prob_no:.2%}")
    print(f"   Probability of Approval: {prob_yes:.2%}")

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,
                            roc_auc_score, roc_curve, precision_score, recall_score, f1_score)
import joblib
import sys
import os


class SuppressPrints:
    def __enter__(self):
        self._original_stdout = sys.stdout
        sys.stdout = open(os.devnull, 'w')
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stdout = self._original_stdout

with SuppressPrints():
    np.random.seed(42)
    n_samples = 100000
    fraud_ratio = 0.002

    n_fraud = int(n_samples * fraud_ratio)
    n_legitimate = n_samples - n_fraud

    # Legitimate transactions
    legitimate_data = {
        'Time': np.random.randint(0, 172800, n_legitimate),
        'V1': np.random.normal(0, 1.5, n_legitimate),
        'V2': np.random.normal(0, 1.5, n_legitimate),
        'V3': np.random.normal(0, 1.5, n_legitimate),
        'V4': np.random.normal(0, 1.5, n_legitimate),
        'V5': np.random.normal(0, 1.5, n_legitimate),
        'V6': np.random.normal(0, 1.5, n_legitimate),
        'V7': np.random.normal(0, 1.5, n_legitimate),
        'V8': np.random.normal(0, 1.5, n_legitimate),
        'V9': np.random.normal(0, 1.5, n_legitimate),
        'V10': np.random.normal(0, 1.5, n_legitimate),
        'Amount': np.random.exponential(50, n_legitimate),
        'Class': np.zeros(n_legitimate)
    }

    # Fraudulent transactions
    fraud_data = {
        'Time': np.random.randint(0, 172800, n_fraud),
        'V1': np.random.normal(2, 2, n_fraud),
        'V2': np.random.normal(-3, 2, n_fraud),
        'V3': np.random.normal(1.5, 2, n_fraud),
        'V4': np.random.normal(-2, 2, n_fraud),
        'V5': np.random.normal(2, 2, n_fraud),
        'V6': np.random.normal(-1, 2, n_fraud),
        'V7': np.random.normal(3, 2, n_fraud),
        'V8': np.random.normal(-2.5, 2, n_fraud),
        'V9': np.random.normal(1, 2, n_fraud),
        'V10': np.random.normal(-3, 2, n_fraud),
        'Amount': np.random.exponential(150, n_fraud),
        'Class': np.ones(n_fraud)
    }

    legitimate_df = pd.DataFrame(legitimate_data)
    fraud_df = pd.DataFrame(fraud_data)
    data = pd.concat([legitimate_df, fraud_df], ignore_index=True)

    data = data.sample(frac=1, random_state=42).reset_index(drop=True)

    plt.ioff()
    fig = plt.figure(figsize=(20, 12))
    plt.close('all')

    df = data.copy()

    X = df.drop('Class', axis=1)
    y = df['Class']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)


    # Model 1: Decision Tree
    dt_model = DecisionTreeClassifier(
        max_depth=10,
        min_samples_split=50,
        min_samples_leaf=20,
        random_state=42,
        class_weight='balanced'
    )
    dt_model.fit(X_train_scaled, y_train)

    # Decision Tree Predictions
    dt_train_pred = dt_model.predict(X_train_scaled)
    dt_test_pred = dt_model.predict(X_test_scaled)
    dt_test_proba = dt_model.predict_proba(X_test_scaled)[:, 1]

    # Decision Tree Metrics
    dt_train_acc = accuracy_score(y_train, dt_train_pred)
    dt_test_acc = accuracy_score(y_test, dt_test_pred)
    dt_precision = precision_score(y_test, dt_test_pred)
    dt_recall = recall_score(y_test, dt_test_pred)
    dt_f1 = f1_score(y_test, dt_test_pred)
    dt_roc_auc = roc_auc_score(y_test, dt_test_proba)
    dt_cv_scores = cross_val_score(dt_model, X_train_scaled, y_train, cv=5)

    # Model 2: Neural Network
    nn_model = MLPClassifier(
        hidden_layer_sizes=(64, 32, 16),
        activation='relu',
        solver='adam',
        learning_rate_init=0.001,
        max_iter=200,
        random_state=42,
        early_stopping=True,
        validation_fraction=0.1,
        verbose=False
    )
    nn_model.fit(X_train_scaled, y_train)

    # Neural Network Predictions
    nn_train_pred = nn_model.predict(X_train_scaled)
    nn_test_pred = nn_model.predict(X_test_scaled)
    nn_test_proba = nn_model.predict_proba(X_test_scaled)[:, 1]

    # Neural Network Metrics
    nn_train_acc = accuracy_score(y_train, nn_train_pred)
    nn_test_acc = accuracy_score(y_test, nn_test_pred)
    nn_precision = precision_score(y_test, nn_test_pred)
    nn_recall = recall_score(y_test, nn_test_pred)
    nn_f1 = f1_score(y_test, nn_test_pred)
    nn_roc_auc = roc_auc_score(y_test, nn_test_proba)
    nn_cv_scores = cross_val_score(nn_model, X_train_scaled, y_train, cv=5)

    # Store results
    results = {
        'Decision Tree': {
            'model': dt_model,
            'train_accuracy': dt_train_acc,
            'test_accuracy': dt_test_acc,
            'precision': dt_precision,
            'recall': dt_recall,
            'f1_score': dt_f1,
            'roc_auc': dt_roc_auc,
            'cv_scores': dt_cv_scores,
            'y_pred_test': dt_test_pred,
            'y_pred_proba': dt_test_proba
        },
        'Neural Network': {
            'model': nn_model,
            'train_accuracy': nn_train_acc,
            'test_accuracy': nn_test_acc,
            'precision': nn_precision,
            'recall': nn_recall,
            'f1_score': nn_f1,
            'roc_auc': nn_roc_auc,
            'cv_scores': nn_cv_scores,
            'y_pred_test': nn_test_pred,
            'y_pred_proba': nn_test_proba
        }
    }

    best_model_name = max(results, key=lambda x: results[x]['test_accuracy'])
    best_model = results[best_model_name]['model']
    best_accuracy = results[best_model_name]['test_accuracy']

    fig = plt.figure(figsize=(18, 12))
    plt.close('all')

    joblib.dump(best_model, 'best_fraud_model.pkl')
    joblib.dump(scaler, 'fraud_scaler.pkl')

# Define prediction function
def predict_fraud(transaction_dict):
    """
    Predict if a transaction is fraudulent

    Parameters:
    transaction_dict: dictionary with transaction features

    Returns:
    prediction, probability of fraud, and risk level
    """
    model = joblib.load('best_fraud_model.pkl')
    scaler = joblib.load('fraud_scaler.pkl')

    # Create DataFrame
    new_df = pd.DataFrame([transaction_dict])

    new_scaled = scaler.transform(new_df)
    prediction = model.predict(new_scaled)[0]

    if hasattr(model, 'predict_proba'):
        probability = model.predict_proba(new_scaled)[0]
        prob_legitimate = probability[0]
        prob_fraud = probability[1]
    else:
        prob_legitimate = prob_fraud = None

    if prob_fraud is not None:
        if prob_fraud < 0.3:
            risk_level = "LOW"
        elif prob_fraud < 0.7:
            risk_level = "MEDIUM"
        else:
            risk_level = "HIGH"
    else:
        risk_level = "UNKNOWN"

    fraud_status = 'FRAUD' if prediction == 1 else 'LEGITIMATE'

    return fraud_status, prob_legitimate, prob_fraud, risk_level


# Test Case 1: Legitimate Transaction
test_case_1 = {
    'Time': 45000,
    'V1': 0.5,
    'V2': -0.3,
    'V3': 0.8,
    'V4': 0.2,
    'V5': -0.4,
    'V6': 0.1,
    'V7': -0.2,
    'V8': 0.3,
    'V9': -0.1,
    'V10': 0.4,
    'Amount': 45.50
}

print("\n Test Case 1: Normal Transaction")
for key, value in test_case_1.items():
    print(f"  {key:<10}: {value}")

result, prob_legit, prob_fraud, risk = predict_fraud(test_case_1)
print(f"\n Prediction: {result} {'✅' if result == 'LEGITIMATE' else '⚠️'}")
if prob_fraud is not None:
    print(f"   Probability of Legitimate: {prob_legit:.2%}")
    print(f"   Probability of Fraud: {prob_fraud:.2%}")
    print(f"   Risk Level: {risk}")

# Test Case 2: Suspicious Transaction
test_case_2 = {
    'Time': 85000,
    'V1': 2.5,
    'V2': -3.2,
    'V3': 1.8,
    'V4': -2.1,
    'V5': 2.3,
    'V6': -1.2,
    'V7': 3.5,
    'V8': -2.8,
    'V9': 1.4,
    'V10': -3.1,
    'Amount': 250.00
}

print("\n Test Case 2: Suspicious Transaction")
for key, value in test_case_2.items():
    print(f"  {key:<10}: {value}")

result, prob_legit, prob_fraud, risk = predict_fraud(test_case_2)
print(f"\n Prediction: {result} {'✅' if result == 'LEGITIMATE' else '⚠️'}")
if prob_fraud is not None:
    print(f"   Probability of Legitimate: {prob_legit:.2%}")
    print(f"   Probability of Fraud: {prob_fraud:.2%}")
    print(f"   Risk Level: {risk}")

# Test Case 3: High-Risk Transaction
test_case_3 = {
    'Time': 120000,
    'V1': 3.2,
    'V2': -4.1,
    'V3': 2.5,
    'V4': -3.0,
    'V5': 3.1,
    'V6': -2.2,
    'V7': 4.0,
    'V8': -3.5,
    'V9': 2.0,
    'V10': -4.2,
    'Amount': 500.00
}

print("\n Test Case 3: High-Value Transaction")
for key, value in test_case_3.items():
    print(f"  {key:<10}: {value}")

result, prob_legit, prob_fraud, risk = predict_fraud(test_case_3)
print(f"\n Prediction: {result} {'✅' if result == 'LEGITIMATE' else '⚠️'}")
if prob_fraud is not None:
    print(f"   Probability of Legitimate: {prob_legit:.2%}")
    print(f"   Probability of Fraud: {prob_fraud:.2%}")
    print(f"   Risk Level: {risk}")

# Test Case 4: Borderline Transaction
test_case_4 = {
    'Time': 60000,
    'V1': 1.2,
    'V2': -1.5,
    'V3': 0.9,
    'V4': -0.8,
    'V5': 1.0,
    'V6': -0.5,
    'V7': 1.3,
    'V8': -1.2,
    'V9': 0.6,
    'V10': -1.4,
    'Amount': 120.00
}

print("\n Test Case 4: Borderline Transaction")
for key, value in test_case_4.items():
    print(f"  {key:<10}: {value}")

result, prob_legit, prob_fraud, risk = predict_fraud(test_case_4)
print(f"\n Prediction: {result} {'✅' if result == 'LEGITIMATE' else '⚠️'}")
if prob_fraud is not None:
    print(f"   Probability of Legitimate: {prob_legit:.2%}")
    print(f"   Probability of Fraud: {prob_fraud:.2%}")
    print(f"   Risk Level: {risk}")

# Test Case 5: Large Amount Transaction
test_case_5 = {
    'Time': 150000,
    'V1': 0.2,
    'V2': -0.1,
    'V3': 0.3,
    'V4': 0.1,
    'V5': -0.2,
    'V6': 0.0,
    'V7': -0.1,
    'V8': 0.2,
    'V9': -0.05,
    'V10': 0.15,
    'Amount': 1500.00
}

print("\n Test Case 5: Large Amount Transaction")
for key, value in test_case_5.items():
    print(f"  {key:<10}: {value}")

result, prob_legit, prob_fraud, risk = predict_fraud(test_case_5)
print(f"\n Prediction: {result} {'✅' if result == 'LEGITIMATE' else '⚠️'}")
if prob_fraud is not None:
    print(f"   Probability of Legitimate: {prob_legit:.2%}")
    print(f"   Probability of Fraud: {prob_fraud:.2%}")
    print(f"   Risk Level: {risk}")