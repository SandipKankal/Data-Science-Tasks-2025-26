# -*- coding: utf-8 -*-
"""ds-defaulter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nG1yGz3U9eaVfBMfIZKcbgUZwV17z90s
"""

# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Read the uploaded dataset
data = pd.read_csv("yield_df.csv")

# View first few rows
data.head()

# Step 4: Check data info and missing values
data.info()
print("\nMissing values:\n", data.isnull().sum())

data.columns

# Step 5: Prepare input and output data safely (handles text columns too)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Target column name
target_col = 'hg/ha_yield'

# Separate features and target
X = data.drop(columns=[target_col])
y = data[target_col]

# Identify categorical (non-numeric) columns
categorical_cols = X.select_dtypes(include=['object']).columns
print("Categorical columns:", list(categorical_cols))

# Encode categorical columns using LabelEncoder
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])
    label_encoders[col] = le

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("‚úÖ Data cleaned, encoded, split, and scaled successfully!")
print(f"Training samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")

# Step 6 + Step 7: Train multiple models and select the best one
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

# Train Linear Regression model
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)

# Evaluate Linear Regression
r2_lr = r2_score(y_test, y_pred_lr)
mse_lr = mean_squared_error(y_test, y_pred_lr)

# Train Random Forest model
rf = RandomForestRegressor(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Evaluate Random Forest
r2_rf = r2_score(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)

# Display comparison
print("üìä Model Performance Comparison:")
print(f"Linear Regression -> R¬≤: {r2_lr:.4f}, MSE: {mse_lr:.4f}")
print(f"Random Forest     -> R¬≤: {r2_rf:.4f}, MSE: {mse_rf:.4f}")

# Choose the best model
if r2_rf > r2_lr:
    best_model = rf
    best_name = "Random Forest Regressor"
else:
    best_model = lr
    best_name = "Linear Regression"

print(f"\nüèÜ Best Model Selected: {best_name}")

# Fi

# Step 8: Visualize Actual vs Predicted Yield
import matplotlib.pyplot as plt

# Predict yield using the final trained model
y_pred_final = best_model.predict(X)

# Create a scatter plot
plt.figure(figsize=(8,5))
plt.scatter(y, y_pred_final, color='green', alpha=0.6)
plt.xlabel("Actual Yield (hg/ha)")
plt.ylabel("Predicted Yield (hg/ha)")
plt.title(f"Actual vs Predicted Crop Yield ({best_name})")
plt.grid(True)
plt.show()

# Step 9: Feature importance (works only if Random Forest is best)
if best_name == "Random Forest Regressor":
    import numpy as np

    feature_importance = best_model.feature_importances_
    features = X.columns

    # Sort and plot
    sorted_idx = np.argsort(feature_importance)[::-1]
    plt.figure(figsize=(8,4))
    plt.barh(range(len(features)), feature_importance[sorted_idx], align='center', color='skyblue')
    plt.yticks(range(len(features)), features[sorted_idx])
    plt.xlabel("Importance")
    plt.title("Feature Importance (Random Forest)")
    plt.gca().invert_yaxis()
    plt.show()
else:
    print("Feature importance is only available for Random Forest.")

# Step 11: Predict on a new sample (replace values as per your dataset)
# Example order of features: ['Unnamed: 0', 'Area', 'Item', 'Year', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']

# Create a sample input (in correct order and format)
sample = pd.DataFrame({
    'Unnamed: 0': [1],
    'Area': [0],  # you can check the label encoding for Area
    'Item': [0],
    'Year': [2025],
    'average_rain_fall_mm_per_year': [1200],
    'pesticides_tonnes': [1500],
    'avg_temp': [26.5]
})

# Make prediction
predicted_yield = best_model.predict(sample)
print(f"üåæ Predicted Crop Yield: {predicted_yield[0]:.2f} hg/ha")

print("--- DataFrame Information (Columns, Counts, Dtypes) ---")
# This shows the column names, non-null counts, and data types
data.info()

print("\n--- List of Column Names ---")
# This gives a simple list of all columns
print(data.columns.tolist())