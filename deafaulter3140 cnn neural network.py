# -*- coding: utf-8 -*-
"""deafaulter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17QayWqFTUrOJ1pWhlpIdbMcmAERRagna
"""

# cnn_fashion_mnist_dataframe.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Load dataset using pandas
train_df = pd.read_csv("/content/fashion-mnist_test.csv")
test_df = pd.read_csv("/content/fashion-mnist_test.csv")

print("Training DataFrame shape:", train_df.shape)
print("Testing DataFrame shape:", test_df.shape)
print(train_df.head())

# Separate labels and features
y_train = train_df["label"].values
y_test = test_df["label"].values

# Drop the label column to get pixel data
x_train = train_df.drop(columns=["label"]).values
x_test = test_df.drop(columns=["label"]).values

# Normalize pixel values to [0,1]
x_train = x_train / 255.0
x_test = x_test / 255.0

# Reshape for CNN: (num_samples, 28, 28, 1)
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)

# ------------------ STEP 2: Class names ------------------
class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ------------------ STEP 3: Visualize few images from dataframe ------------------
plt.figure(figsize=(8,8))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(x_train[i].reshape(28,28), cmap="gray")
    plt.title(class_names[y_train[i]])
    plt.axis("off")
plt.show()

# ------------------ STEP 4: Build the CNN ------------------
def make_model():
    inp = layers.Input(shape=(28,28,1))
    x = layers.Conv2D(32, (3,3), padding="same", activation="relu")(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3,3), padding="same", activation="relu")(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(0.25)(x)

    x = layers.Conv2D(64, (3,3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3,3), padding="same", activation="relu")(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(0.25)(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)
    out = layers.Dense(10, activation="softmax")(x)
    model = models.Model(inputs=inp, outputs=out)
    return model

model = make_model()
model.summary()

# ------------------ STEP 5: Compile ------------------
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# ------------------ STEP 6: Callbacks ------------------
callbacks = [
    EarlyStopping(monitor="val_loss", patience=6, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6, verbose=1),
    ModelCheckpoint("best_fashion_cnn.h5", monitor="val_loss", save_best_only=True, verbose=1),
]

# ------------------ STEP 7: Data Augmentation ------------------
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.08,
    height_shift_range=0.08,
    shear_range=0.1,
    zoom_range=0.08
)
datagen.fit(x_train)

# ------------------ STEP 8: Train the Model ------------------
BATCH_SIZE = 128
EPOCHS = 20

history = model.fit(
    datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),
    epochs=EPOCHS,
    steps_per_epoch=x_train.shape[0] // BATCH_SIZE,
    validation_data=(x_test, y_test),
    callbacks=callbacks,
    verbose=2
)

# ------------------ STEP 9: Plot Training Curves ------------------
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend(); plt.title('Loss')

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend(); plt.title('Accuracy')
plt.show()

# ------------------ STEP 10: Evaluate and Predictions ------------------
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}")

y_pred_probs = model.predict(x_test)
y_pred = np.argmax(y_pred_probs, axis=1)

print("\nClassification report:")
print(classification_report(y_test, y_pred, target_names=class_names))

# ------------------ STEP 11: Confusion Matrix ------------------
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names, cmap="Blues")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.title("Confusion Matrix")
plt.show()

# ------------------ STEP 12: Visualize Predictions ------------------
num = 12
plt.figure(figsize=(12,8))
for i in range(num):
    plt.subplot(3,4,i+1)
    plt.imshow(x_test[i].reshape(28,28), cmap='gray')
    plt.title(f"T:{class_names[y_test[i]]}\nP:{class_names[y_pred[i]]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# ------------------ STEP 13: Save the Model ------------------
model.save("fashion_mnist_dataframe_cnn.h5")
print(" Model saved as fashion_mnist_dataframe_cnn.h5")

# cnn_fashion_mnist_dataframe_custom.py

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2

# ------------------ STEP 1: Load dataset using pandas ------------------
train_df = pd.read_csv("/content/fashion-mnist_train.csv")  # Replace with correct CSV path
test_df = pd.read_csv("/content/fashion-mnist_test.csv")

print("Training DataFrame shape:", train_df.shape)
print("Testing DataFrame shape:", test_df.shape)
print(train_df.head())

# Separate labels and features
y_train = train_df["label"].values
y_test = test_df["label"].values

x_train = train_df.drop(columns=["label"]).values
x_test = test_df.drop(columns=["label"]).values

# Normalize pixel values
x_train = x_train / 255.0
x_test = x_test / 255.0

# Reshape for CNN
x_train = x_train.reshape(-1, 28, 28, 1)
x_test = x_test.reshape(-1, 28, 28, 1)

print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)

# ------------------ STEP 2: Class names ------------------
class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ------------------ STEP 3: Visualize few images ------------------
plt.figure(figsize=(8,8))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(x_train[i].reshape(28,28), cmap="gray")
    plt.title(class_names[y_train[i]])
    plt.axis("off")
plt.show()

# ------------------ STEP 4: Build the CNN ------------------
def make_model():
    inp = layers.Input(shape=(28,28,1))

    x = layers.Conv2D(32, (3,3), padding="same", activation="relu")(inp)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(32, (3,3), padding="same", activation="relu")(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(0.25)(x)

    x = layers.Conv2D(64, (3,3), padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(64, (3,3), padding="same", activation="relu")(x)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Dropout(0.25)(x)

    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.4)(x)

    out = layers.Dense(10, activation="softmax")(x)

    model = models.Model(inputs=inp, outputs=out)
    return model

model = make_model()
model.summary()

# ------------------ STEP 5: Compile ------------------
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

# ------------------ STEP 6: Callbacks ------------------
callbacks = [
    EarlyStopping(monitor="val_loss", patience=6, restore_best_weights=True),
    ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-6, verbose=1),
    ModelCheckpoint("best_fashion_cnn.h5", monitor="val_loss", save_best_only=True, verbose=1)
]

# ------------------ STEP 7: Data Augmentation ------------------
datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.08,
    height_shift_range=0.08,
    shear_range=0.1,
    zoom_range=0.08
)
datagen.fit(x_train)

# ------------------ STEP 8: Train the Model ------------------
BATCH_SIZE = 128
EPOCHS = 20

history = model.fit(
    datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),
    epochs=EPOCHS,
    steps_per_epoch=x_train.shape[0] // BATCH_SIZE,
    validation_data=(x_test, y_test),
    callbacks=callbacks,
    verbose=2
)

# ------------------ STEP 9: Plot Training Curves ------------------
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='train loss')
plt.plot(history.history['val_loss'], label='val loss')
plt.legend(); plt.title('Loss')

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='train acc')
plt.plot(history.history['val_accuracy'], label='val acc')
plt.legend(); plt.title('Accuracy')
plt.show()

# ------------------ STEP 10: Evaluate and Predictions ------------------
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}")

y_pred_probs = model.predict(x_test)
y_pred = np.argmax(y_pred_probs, axis=1)

print("\nClassification report:")
print(classification_report(y_test, y_pred, target_names=class_names))

# ------------------ STEP 11: Confusion Matrix ------------------
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt="d", xticklabels=class_names, yticklabels=class_names, cmap="Blues")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.title("Confusion Matrix")
plt.show()

# ------------------ STEP 12: Visualize Predictions ------------------
num = 12
plt.figure(figsize=(12,8))
for i in range(num):
    plt.subplot(3,4,i+1)
    plt.imshow(x_test[i].reshape(28,28), cmap='gray')
    plt.title(f"T:{class_names[y_test[i]]}\nP:{class_names[y_pred[i]]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# ------------------ STEP 13: Save the Model ------------------
model.save("fashion_mnist_dataframe_cnn.h5")
print("Model saved as fashion_mnist_dataframe_cnn.h5")

# ------------------ STEP 14: Predict Custom Image ------------------
def predict_custom_image(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale
    img = cv2.resize(img, (28, 28))                   # Resize to 28x28
    img = img / 255.0                                 # Normalize
    img = img.reshape(1, 28, 28, 1)                   # Reshape for model input

    predictions = model.predict(img)
    predicted_class = np.argmax(predictions)
    print(f"\nPredicted Category: {class_names[predicted_class]}")

    plt.imshow(img.reshape(28,28), cmap="gray")
    plt.title(f"Predicted: {class_names[predicted_class]}")
    plt.axis("off")
    plt.show()

predict_custom_image("/content/images.jfif")

import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.keras.models import load_model

# ------------------ Load trained model ------------------
model = load_model("fashion_mnist_dataframe_cnn.h5")

# ------------------ Class names ------------------
class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# ------------------ Predict a single image ------------------
def predict_custom_image(img_path):
    # 1️⃣ Load image
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # convert to grayscale

    if img is None:
        print(f"Image not found at {img_path}")
        return

    # 2️⃣ Resize to 28x28
    img = cv2.resize(img, (28, 28))

    # 3️⃣ Normalize
    img = img / 255.0

    # 4️⃣ Reshape to (1, 28, 28, 1)
    img_input = img.reshape(1, 28, 28, 1)

    # 5️⃣ Predict
    predictions = model.predict(img_input)
    predicted_class = np.argmax(predictions)

    # 6️⃣ Print and show
    print(f"Predicted Category: {class_names[predicted_class]}")

    plt.imshow(img, cmap="gray")
    plt.title(f"Predicted: {class_names[predicted_class]}")
    plt.axis("off")
    plt.show()


# ------------------ Example ------------------
predict_custom_image("/content/images.jfif")