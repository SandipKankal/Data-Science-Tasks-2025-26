# -*- coding: utf-8 -*-
"""202001103086 DS Defaulter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kmjoXnM2iiNK-eiLYvnx1m1uZ-6Mqq6Q
"""

# 1. Import libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# 2. Create dummy data (replace this with your actual dataset)
np.random.seed(42)
data_size = 1000

data = pd.DataFrame({
    'region': np.random.choice(['North', 'South', 'East', 'West'], size=data_size),
    'product_category': np.random.choice(['Electronics', 'Furniture', 'Automotive'], size=data_size),
    'claim_value': np.random.uniform(100, 5000, size=data_size),
    'customer_age': np.random.randint(18, 70, size=data_size),
    'claim_duration': np.random.randint(1, 30, size=data_size),
    'is_fraud': np.random.choice([0, 1], size=data_size, p=[0.85, 0.15])  # Assume 15% fraud
})

# 3. Preprocess data
categorical_cols = ['region', 'product_category']
numerical_cols = ['claim_value', 'customer_age', 'claim_duration']

# Encode categorical columns
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# Features and labels
X = data[categorical_cols + numerical_cols]
y = data['is_fraud']

# Scale numerical data
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# CNN requires 3D input: (samples, timesteps, features)
# We'll treat each feature as a "time step" with 1 feature
X_train_cnn = np.expand_dims(X_train.values, axis=2)
X_test_cnn = np.expand_dims(X_test.values, axis=2)

# 4. Build the CNN model
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# 5. Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 6. Train model
model.fit(X_train_cnn, y_train, epochs=20, batch_size=32, validation_split=0.1)

# 7. Evaluate
loss, accuracy = model.evaluate(X_test_cnn, y_test)
print(f"Test Accuracy: {accuracy:.2f}")