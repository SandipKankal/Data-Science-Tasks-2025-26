
# -*- coding: utf-8 -*-
"""Untitled71.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lw0X2JhhLEpD9Z9BJvqim3DcrlCV3zRB
"""



#1

# loan_prediction_with_uploaded_data.py
# Requirements: pandas, numpy, scikit-learn, joblib
# pip install pandas numpy scikit-learn joblib

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import os

# ========== 1. Load data ==========
data_path = "loan.csv"
if not os.path.exists(data_path):
    raise FileNotFoundError(f"{data_path} not found. Update path if needed.")
df = pd.read_csv(data_path)

# ========== 2. Quick cleanup ==========
# Convert '3+' -> 3 in Dependents and numericify
if "Dependents" in df.columns:
    df["Dependents"] = df["Dependents"].replace("3+", "3")
    df["Dependents"] = pd.to_numeric(df["Dependents"], errors="coerce")

# Map target
target_col = "Loan_Status"
if target_col not in df.columns:
    raise ValueError(f"Target column '{target_col}' not found in CSV.")
df = df[df[target_col].notna()]  # drop rows without target
df[target_col] = df[target_col].map({"Y": 1, "N": 0})

# Optional: drop Loan_ID (identifier)
if "Loan_ID" in df.columns:
    df = df.drop(columns=["Loan_ID"])

# ========== 3. Select features ==========
numeric_features = ["ApplicantIncome", "CoapplicantIncome", "LoanAmount", "Loan_Amount_Term", "Dependents"]
numeric_features = [c for c in numeric_features if c in df.columns]

categorical_features = ["Gender", "Married", "Education", "Self_Employed", "Credit_History", "Property_Area"]
categorical_features = [c for c in categorical_features if c in df.columns]

features = numeric_features + categorical_features
if not features:
    raise ValueError("No features found in the dataset using the default feature list. Update features list.")
print("Using features:", features)

X = df[features].copy()
y = df[target_col].astype(int).copy()

# ========== 4. Preprocessing pipelines ==========
numeric_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    # use OneHotEncoder for categorical variables (handle unknowns)
    ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
])

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, numeric_features),
    ("cat", cat_pipeline, categorical_features)
])

# ========== 5. Train-test split ==========
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

# Fit preprocessor
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

# ========== 6. Train models ==========
# Logistic Regression
log_clf = LogisticRegression(max_iter=1000, random_state=42)
log_clf.fit(X_train_prep, y_train)
y_pred_log = log_clf.predict(X_test_prep)
acc_log = accuracy_score(y_test, y_pred_log)
print(f"Logistic Regression accuracy: {acc_log:.4f}")

# Decision Tree with small grid search (use n_jobs=1 to avoid parallel import issues)
dt = DecisionTreeClassifier(random_state=42)
params = {"max_depth": [3, 5, 7, None], "min_samples_split": [2, 5, 10]}
grid = GridSearchCV(dt, params, cv=5, scoring="accuracy", n_jobs=1)  # n_jobs=1 is safer in some environments
grid.fit(X_train_prep, y_train)
best_dt = grid.best_estimator_
y_pred_dt = best_dt.predict(X_test_prep)
acc_dt = accuracy_score(y_test, y_pred_dt)
print("Decision Tree best params:", grid.best_params_)
print(f"Decision Tree accuracy: {acc_dt:.4f}")

# ========== 7. Choose best ==========
if acc_log >= acc_dt:
    best_model = log_clf
    best_name = "LogisticRegression"
    best_acc = acc_log
else:
    best_model = best_dt
    best_name = "DecisionTree"
    best_acc = acc_dt

print(f"Best model: {best_name} (accuracy={best_acc:.4f})")

# ========== 8. Evaluation ==========
y_pred_best = best_model.predict(X_test_prep)
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_best))
print("\nClassification Report:\n", classification_report(y_test, y_pred_best))

# ========== 9. Save pipeline ==========
final_pipeline = Pipeline([("preprocessor", preprocessor), ("model", best_model)])
output_path = "best_loan_model.pkl"
joblib.dump(final_pipeline, output_path)
print(f"\nSaved pipeline to: {output_path}")

# ========== 10. Example prediction ==========
example = {
    "ApplicantIncome": 5000,
    "CoapplicantIncome": 0.0,
    "LoanAmount": 128.0,
    "Loan_Amount_Term": 360.0,
    "Dependents": 0,
    "Gender": "Male",
    "Married": "No",
    "Education": "Graduate",
    "Self_Employed": "No",
    "Credit_History": 1.0,
    "Property_Area": "Urban"
}
sample = pd.DataFrame([{k: example.get(k, np.nan) for k in features}])
loaded = joblib.load(output_path)
pred = loaded.predict(sample)[0]
proba = loaded.predict_proba(sample)[0] if hasattr(loaded, "predict_proba") else None
print("\nExample prediction (1=approved,0=not):", pred)
if proba is not None:
    print("Probabilities:", proba)

# Credit Card Fraud Detection: Decision Tree vs Neural Network (MLP)
# ---------------------------------------------------------------

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix, classification_report
)
import warnings
warnings.filterwarnings("ignore")

# ---------------------------------------------------------------
# 1. Load dataset
# ---------------------------------------------------------------
df = pd.read_csv("creditcard.csv")   # <-- make sure this file is in the same folder
print("Full dataset shape:", df.shape)
print("Class distribution:\n", df['Class'].value_counts())

# ---------------------------------------------------------------
# 2. Undersample for faster training
# ---------------------------------------------------------------
frauds = df[df['Class'] == 1]
nonfrauds = df[df['Class'] == 0].sample(n=5000, random_state=42)  # pick 5k non-frauds
df_small = pd.concat([frauds, nonfrauds]).sample(frac=1, random_state=42).reset_index(drop=True)

print("\nDataset used for training/testing:")
print("Shape:", df_small.shape)
print("Class distribution:\n", df_small['Class'].value_counts())

# ---------------------------------------------------------------
# 3. Split features and target
# ---------------------------------------------------------------
X = df_small.drop(columns=['Class'])
y = df_small['Class']

# Scale 'Time' and 'Amount' columns
scaler = StandardScaler()
X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])

# Train/test split (stratified)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
print("\nTrain set:", X_train.shape, "Test set:", X_test.shape)

# ---------------------------------------------------------------
# 4. Helper function for evaluation
# ---------------------------------------------------------------
def evaluate_model(name, model, X_test, y_test):
    preds = model.predict(X_test)
    probs = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, zero_division=0)
    rec = recall_score(y_test, preds, zero_division=0)
    f1 = f1_score(y_test, preds, zero_division=0)
    roc = roc_auc_score(y_test, probs) if probs is not None else np.nan

    print(f"\n{name} Results:")
    print("Accuracy:  {:.6f}".format(acc))
    print("Precision: {:.6f}".format(prec))
    print("Recall:    {:.6f}".format(rec))
    print("F1-score:  {:.6f}".format(f1))
    print("ROC AUC:   {:.6f}".format(roc))
    print("Confusion Matrix:\n", confusion_matrix(y_test, preds))

    return {'Model': name, 'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1': f1, 'ROC_AUC': roc}

# ---------------------------------------------------------------
# 5. Train and evaluate models
# ---------------------------------------------------------------
# Decision Tree
dt = DecisionTreeClassifier(max_depth=6, class_weight='balanced', random_state=42)
dt.fit(X_train, y_train)
dt_res = evaluate_model("Decision Tree", dt, X_test, y_test)

# Neural Network (MLP)
mlp = MLPClassifier(hidden_layer_sizes=(100,), alpha=0.0005, max_iter=300,
                    early_stopping=True, random_state=42)
mlp.fit(X_train, y_train)
mlp_res = evaluate_model("Neural Network (MLP)", mlp, X_test, y_test)

# ---------------------------------------------------------------
# 6. Compare results
# ---------------------------------------------------------------
results = pd.DataFrame([dt_res, mlp_res])
print("\nModel comparison:")
print(results)

best = results.loc[results['Accuracy'].idxmax()]
print(f"\nBest Model: {best['Model']} (Accuracy: {best['Accuracy']:.6f})")

# Detailed classification report for best model
best_model = dt if best['Model'] == "Decision Tree" else mlp
print("\nClassification Report for Best Model:\n")
print(classification_report(y_test, best_model.predict(X_test), digits=6))

#3
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error

# Step 1: Load the dataset
df = pd.read_csv('yield.csv')  # Make sure 'yeild.csv' is in your working directory

# Step 2: Clean and preprocess
# Drop irrelevant columns if they exist
drop_cols = ['Domain Code', 'Domain', 'Area Code', 'Element Code', 'Item Code', 'Year Code', 'Unit']
df = df.drop(columns=[col for col in drop_cols if col in df.columns])

# Encode categorical features
le = LabelEncoder()
for col in ['Area', 'Element', 'Item']:
    if df[col].dtype == 'object':
        df[col] = le.fit_transform(df[col])

# Features and target
X = df.drop(columns=['Value'])
y = df['Value']

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 3: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Step 4: Train models
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)
dt_preds = dt_model.predict(X_test)

mlp_model = MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
mlp_model.fit(X_train, y_train)
mlp_preds = mlp_model.predict(X_test)

# Step 5: Evaluate models
dt_r2 = r2_score(y_test, dt_preds)
dt_mse = mean_squared_error(y_test, dt_preds)

mlp_r2 = r2_score(y_test, mlp_preds)
mlp_mse = mean_squared_error(y_test, mlp_preds)

print("📊 Decision Tree Regressor:")
print(f"R² Score: {dt_r2:.4f}")
print(f"MSE: {dt_mse:.2f}")

print("\n🤖 MLP Regressor:")
print(f"R² Score: {mlp_r2:.4f}")
print(f"MSE: {mlp_mse:.2f}")

# Step 6: Select best model and show predictions
best_model = dt_model if dt_r2 > mlp_r2 else mlp_model
best_name = "Decision Tree" if dt_r2 > mlp_r2 else "MLP Regressor"

print(f"\n🏆 Best Model: {best_name}")
print("🔮 Sample Predictions:")
sample_input = X_test[:3]
sample_preds = best_model.predict(sample_input)
for i, pred in enumerate(sample_preds):
    print(f"Sample {i+1}: Predicted Yield = {pred:.2f} hg/ha")




Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Gkjm4rpqDHf32RKiRlBhI9QwnhfvjWi
"""

!pip install pandas
!pip install scikit-learn
!pip install matplotlib
!pip install seaborn

from google.colab import files
uploaded = files.upload()

import pandas as pd

data = pd.read_csv('/content/df_Clean.csv')
print(data.head())
print(data.columns)

# Select features and target
categorical_cols = ['Region', 'State', 'Area', 'City', 'Consumer_profile',
                    'Product_category', 'Product_type', 'Purchased_from', 'Purpose']

numerical_cols = ['AC_1001_Issue', 'AC_1002_Issue', 'AC_1003_Issue',
                  'TV_2001_Issue', 'TV_2002_Issue', 'TV_2003_Issue',
                  'Claim_Value', 'Service_Centre', 'Product_Age', 'Call_details']

X = data[categorical_cols + numerical_cols]
y = data['Fraud']

# One-hot encode categorical columns
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

print(X.head())

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
