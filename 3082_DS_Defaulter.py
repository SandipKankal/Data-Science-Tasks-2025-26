# -*- coding: utf-8 -*-
"""logistic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rkwzr--KKS6UW_2zVHL4BWuAJHrpFDv8
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

file_name = '/content/cirrhosis.csv' # Assuming the file is uploaded or accessible

try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Please ensure it is uploaded to Colab.")
    exit()

# --- 2. Data Preprocessing ---

# Create binary target: 'D' (Dead) -> 1, 'C'/'CL' (Censored) -> 0
df['Target'] = df['Status'].apply(lambda x: 1 if x == 'D' else 0)
df.drop(['ID', 'Status'], axis=1, inplace=True) # Drop ID and original Status

# Identify columns for imputation
categorical_cols = ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders']
numerical_cols = ['Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']

# Impute missing values (using mode for categorical, median for numerical)
for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

for col in numerical_cols:
    df[col].fillna(df[col].median(), inplace=True)

# One-Hot Encoding for remaining categorical features
categorical_features_for_ohe = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']
df_encoded = pd.get_dummies(df, columns=categorical_features_for_ohe, drop_first=True)

# Separate features (X) and target (y)
X = df_encoded.drop('Target', axis=1)
y = df_encoded['Target']

# Identify numerical columns for scaling (all non-dummies, non-target)
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
# Exclude the new dummy/encoded columns (which are uint8) if they are in the list by mistake.
numerical_features = [col for col in numerical_features if X[col].nunique() > 2]

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale numerical features
scaler = StandardScaler()
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

# --- 3. Train Logistic Regression Model ---
model = LogisticRegression(random_state=42, max_iter=1000)
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# --- 4. Evaluate Model ---
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
report = classification_report(y_test, y_pred, target_names=['C/CL (0)', 'D (1)'])

print("\n" + "="*50)
print(f"LOGISTIC REGRESSION PERFORMANCE")
print("="*50)
print(f"Accuracy: {accuracy:.4f}\n")
print("Classification Report:\n", report)

# --- 5. Plot Confusion Matrix ---
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted 0 (C/CL)', 'Predicted 1 (D)'],
            yticklabels=['Actual 0 (C/CL)', 'Actual 1 (D)'])
plt.title('Confusion Matrix for Logistic Regression')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show() # Use plt.show() in Colab to display the plot immediately

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Load the Dataset ---
# You can upload the file by running the following in a separate Colab cell first:
# from google.colab import files
# uploaded = files.upload()
# file_name = list(uploaded.keys())[0]
file_name = 'cirrhosis.csv' # Assuming the file is uploaded or accessible

try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Please ensure it is uploaded to Colab.")
    # Exit or provide a sample data creation if you need to run the code block fully in Colab
    # For this example, we'll assume the file is present.

# --- 2. Data Preprocessing ---

# Create binary target: 'D' (Dead) -> 1, 'C'/'CL' (Censored) -> 0
df['Target'] = df['Status'].apply(lambda x: 1 if x == 'D' else 0)
df.drop(['ID', 'Status'], axis=1, inplace=True) # Drop ID and original Status

# Identify columns for imputation
categorical_cols = ['Drug', 'Ascites', 'Hepatomegaly', 'Spiders']
numerical_cols = ['Cholesterol', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']

# Impute missing values (using mode for categorical, median for numerical)
for col in categorical_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

for col in numerical_cols:
    df[col].fillna(df[col].median(), inplace=True)

# One-Hot Encoding for remaining categorical features
categorical_features_for_ohe = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']
df_encoded = pd.get_dummies(df, columns=categorical_features_for_ohe, drop_first=True)

# Separate features (X) and target (y)
X = df_encoded.drop('Target', axis=1)
y = df_encoded['Target']

# Identify numerical columns for scaling
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
numerical_features = [col for col in numerical_features if X[col].nunique() > 2] # Exclude binary encoded columns

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale numerical features (Crucial for distance-based models, less so for DT, but good practice for consistency)
scaler = StandardScaler()
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])
X_test[numerical_features] = scaler.transform(X_test[numerical_features])

# --- 3. Train Decision Tree Model ---
# Limiting max_depth for a readable visualization
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_model.fit(X_train, y_train)

# Predict
y_pred_dt = dt_model.predict(X_test)

# --- 4. Evaluate Model ---
accuracy_dt = accuracy_score(y_test, y_pred_dt)
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)
report_dt = classification_report(y_test, y_pred_dt, target_names=['C/CL (0)', 'D (1)'])

print("\n" + "="*60)
print(f"DECISION TREE PERFORMANCE (Max Depth: 5)")
print("="*60)
print(f"Accuracy: {accuracy_dt:.4f}\n")
print("Classification Report:\n", report_dt)

# --- 5. Plot Decision Tree Visualization ---
feature_names = X_train.columns.tolist()
class_names = ['C/CL (0)', 'D (1)']

plt.figure(figsize=(25, 20))
plot_tree(dt_model,
          feature_names=feature_names,
          class_names=class_names,
          filled=True,
          rounded=True,
          fontsize=10)
plt.title('Decision Tree Visualization (Max Depth: 5)', fontsize=16)
plt.tight_layout()
plt.show() # Use plt.show() for Colab display

# --- 6. Plot Confusion Matrix ---
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Reds', cbar=False,
            xticklabels=['Predicted 0 (C/CL)', 'Predicted 1 (D)'],
            yticklabels=['Actual 0 (C/CL)', 'Actual 1 (D)'])
plt.title('Confusion Matrix for Decision Tree')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.tight_layout()
plt.show() # Use plt.show() for Colab display

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# --- 1. Load the Dataset ---
# You can upload the file using Colab's file upload tool.
# In a separate Colab cell, you would typically run:
# from google.colab import files
# uploaded = files.upload()
# file_name = list(uploaded.keys())[0]

file_name = 'cirrhosis.csv' # Assumes the file is uploaded or accessible

try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Please ensure it is uploaded to Colab.")
    # You might want to halt execution here in a real scenario.

# --- 2. Data Preparation for Comparison Plots ---

# Create a simplified target status for binary comparison: 'D' vs 'C/CL'
df['Binary_Status'] = df['Status'].apply(lambda x: 'Dead (D)' if x == 'D' else 'Censored (C/CL)')

# Identify key numerical columns for plotting
numerical_cols_to_check = ['Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']

# Impute missing numerical values with the median for plotting consistency
for col in numerical_cols_to_check:
    df[col].fillna(df[col].median(), inplace=True)

# Convert Stage to integer for the count plot
df['Stage'] = df['Stage'].astype(int)

print("\nGenerating Comparison Plots...")

# --- 3. Generate Comparison Plots (using plt.show() for Colab display) ---

## Plot 1: Age Distribution
plt.figure(figsize=(8, 6))
sns.boxplot(x='Binary_Status', y='Age', data=df)
plt.title('Age Distribution by Patient Status')
plt.ylabel('Age (days)')
plt.xlabel('Patient Status')
plt.tight_layout()
plt.show()

## Plot 2: Bilirubin Distribution
plt.figure(figsize=(8, 6))
sns.boxplot(x='Binary_Status', y='Bilirubin', data=df)
plt.title('Bilirubin Distribution by Patient Status')
plt.ylabel('Bilirubin')
plt.xlabel('Patient Status')
plt.tight_layout()
plt.show()

## Plot 3: Albumin Distribution
plt.figure(figsize=(8, 6))
sns.boxplot(x='Binary_Status', y='Albumin', data=df)
plt.title('Albumin Distribution by Patient Status')
plt.ylabel('Albumin')
plt.xlabel('Patient Status')
plt.tight_layout()
plt.show()

## Plot 4: Status Distribution by Cirrhosis Stage
plt.figure(figsize=(10, 6))
# Ensure Stage is treated as categorical for the countplot x-axis
sns.countplot(x='Stage', hue='Binary_Status', data=df, palette='Set1')
plt.title('Patient Status Distribution by Cirrhosis Stage')
plt.ylabel('Count')
plt.xlabel('Stage')
plt.legend(title='Status')
plt.tight_layout()
plt.show()

print("\nComparison plots displayed successfully.")

#Random forest
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# --- 1. Data Loading ---
# Assuming 'cirrhosis.csv' has been uploaded to the Colab environment
file_name = 'cirrhosis.csv'
try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    exit()

# --- 2. Data Preprocessing ---

# Define Target: We will predict 'Stage'
# Drop rows where 'Stage' is missing (only 6 records are missing this value)
df.dropna(subset=['Stage'], inplace=True)
df['Stage'] = df['Stage'].astype(int) # Ensure Stage is an integer

# Define Features (X) and Target (y)
X = df.drop(['ID', 'Status', 'Stage'], axis=1) # Drop ID, Status (as Stage is new target), and the target itself
y = df['Stage']

# Separate column types for imputation and encoding
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()

# Impute Missing Values: Mode for categorical, Median for numerical
for col in categorical_cols:
    X[col].fillna(X[col].mode()[0], inplace=True)

for col in numerical_cols:
    X[col].fillna(X[col].median(), inplace=True)

# One-Hot Encoding for categorical features
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)

# Scaling (Optional for Random Forest, but consistent pre-processing)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame to keep feature names
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# --- 3. Train Random Forest Model ---
# Use 100 trees and set random_state for reproducibility
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train_scaled, y_train)

# Predict
y_pred_rf = rf_model.predict(X_test_scaled)

# --- 4. Evaluate Model ---
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("\n" + "="*50)
print(f"RANDOM FOREST CLASSIFICATION PERFORMANCE (Target: Stage)")
print("="*50)
print(f"Accuracy: {accuracy_rf:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

# --- 5. Plot Feature Importance ---

# Get feature importance scores
feature_importances = pd.Series(rf_model.feature_importances_, index=X_encoded.columns)
# Select top 15 features for clarity
top_features = feature_importances.nlargest(15)

plt.figure(figsize=(10, 8))
sns.barplot(x=top_features.values, y=top_features.index, palette='coolwarm')
plt.title('Top 15 Feature Importances for Stage Prediction (Random Forest)', fontsize=14)
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show() # Display plot in Colab

# --- 6. Plot Confusion Matrix ---
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Oranges', cbar=False,
            xticklabels=np.unique(y),
            yticklabels=np.unique(y))
plt.title('Confusion Matrix for Random Forest (Predicting Stage)')
plt.ylabel('Actual Stage')
plt.xlabel('Predicted Stage')
plt.tight_layout()
plt.show() # Display plot in Colab

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer

# --- 1. Data Loading ---
# Assuming 'cirrhosis.csv' has been uploaded to the Colab environment
file_name = 'cirrhosis.csv'
try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    exit()

# Drop rows where 'Stage' is missing as it is crucial for both tasks
df.dropna(subset=['Stage'], inplace=True)

# Define initial features
X_initial = df.drop(['ID', 'Status'], axis=1)

# Separate column types for imputation and encoding
categorical_cols = X_initial.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X_initial.select_dtypes(include=np.number).columns.tolist()

# --- 2. Comprehensive Preprocessing (Imputation & Encoding) ---

# Impute Missing Values
# Mode for categorical
for col in categorical_cols:
    X_initial[col].fillna(X_initial[col].mode()[0], inplace=True)
# Median for numerical
for col in numerical_cols:
    X_initial[col].fillna(X_initial[col].median(), inplace=True)

# One-Hot Encoding for categorical features
X_encoded = pd.get_dummies(X_initial.drop('Stage', axis=1), columns=categorical_cols, drop_first=True)
X_encoded['Stage'] = X_initial['Stage'].astype(int) # Add Stage back for target separation

# ----------------------------------------------------------------------
#                         TASK 1: BINARY CLASSIFICATION (Diagnosis)
# ----------------------------------------------------------------------

# Target: 0 (Normal/Early) = Stage 1 or 2, 1 (Advanced) = Stage 3 or 4
df['Diagnosis_Target'] = np.where(df['Stage'].isin([3, 4]), 1, 0)
y_binary = df.loc[X_encoded.index]['Diagnosis_Target'] # Align index after dropping NaNs
X_binary = X_encoded.drop('Stage', axis=1)

# Split and Scale for Binary Task
X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(
    X_binary, y_binary, test_size=0.2, random_state=42, stratify=y_binary
)

scaler_bin = StandardScaler()
X_train_bin_scaled = scaler_bin.fit_transform(X_train_bin)
X_test_bin_scaled = scaler_bin.transform(X_test_bin)

# Train SVM for Binary Classification (RBF kernel is often a good default)
svm_bin = SVC(kernel='rbf', random_state=42, class_weight='balanced')
svm_bin.fit(X_train_bin_scaled, y_train_bin)

# Predict and Evaluate Binary
y_pred_bin = svm_bin.predict(X_test_bin_scaled)
accuracy_bin = accuracy_score(y_test_bin, y_pred_bin)
conf_matrix_bin = confusion_matrix(y_test_bin, y_pred_bin)

print("\n" + "="*70)
print(f"SVM TASK 1: BINARY CLASSIFICATION (Early vs. Advanced Cirrhosis)")
print("="*70)
print(f"Accuracy: {accuracy_bin:.4f}\n")
print("Classification Report:\n", classification_report(y_test_bin, y_pred_bin, target_names=['Stage 1/2 (0)', 'Stage 3/4 (1)']))

# Plot Confusion Matrix for Binary Task
plt.figure(figsize=(7, 6))
sns.heatmap(conf_matrix_bin, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Predicted Stage 1/2 (0)', 'Predicted Stage 3/4 (1)'],
            yticklabels=['Actual Stage 1/2 (0)', 'Actual Stage 3/4 (1)'])
plt.title('SVM Confusion Matrix: Early vs. Advanced Diagnosis')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.tight_layout()
plt.show()

# ----------------------------------------------------------------------
#                         TASK 2: MULTI-CLASS CLASSIFICATION (Staging)
# ----------------------------------------------------------------------

y_multi = X_encoded['Stage']
X_multi = X_encoded.drop('Stage', axis=1)

# Split and Scale for Multi-class Task
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi
)

# Use the same scaling logic
scaler_multi = StandardScaler()
X_train_multi_scaled = scaler_multi.fit_transform(X_train_multi)
X_test_multi_scaled = scaler_multi.transform(X_test_multi)

# Train SVM for Multi-class Classification (Uses One-vs-One or One-vs-Rest by default)
# We use 'rbf' kernel and 'balanced' weights due to class imbalance in stages
svm_multi = SVC(kernel='rbf', random_state=42, class_weight='balanced')
svm_multi.fit(X_train_multi_scaled, y_train_multi)

# Predict and Evaluate Multi-class
y_pred_multi = svm_multi.predict(X_test_multi_scaled)
accuracy_multi = accuracy_score(y_test_multi, y_pred_multi)
conf_matrix_multi = confusion_matrix(y_test_multi, y_pred_multi)

print("\n" + "="*70)
print(f"SVM TASK 2: MULTI-CLASS CLASSIFICATION (Predicting Stage 1-4)")
print("="*70)
print(f"Accuracy: {accuracy_multi:.4f}\n")
print("Classification Report:\n", classification_report(y_test_multi, y_pred_multi))

# Plot Confusion Matrix for Multi-class Task
plt.figure(figsize=(8, 7))
sns.heatmap(conf_matrix_multi, annot=True, fmt='d', cmap='Oranges', cbar=False,
            xticklabels=sorted(y_multi.unique()),
            yticklabels=sorted(y_multi.unique()))
plt.title('SVM Confusion Matrix: Cirrhosis Stage Prediction')
plt.ylabel('Actual Stage')
plt.xlabel('Predicted Stage')
plt.tight_layout()
plt.show()

#KNN algorithm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import cross_val_score

# --- 1. Data Loading ---
# Assuming 'cirrhosis.csv' has been uploaded to the Colab environment
file_name = 'cirrhosis.csv'
try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found.")
    exit()

# Drop rows where 'Stage' is missing as it is the target
df.dropna(subset=['Stage'], inplace=True)
df['Stage'] = df['Stage'].astype(int) # Ensure Stage is an integer

# Define Features (X) and Target (y)
X = df.drop(['ID', 'Status', 'Stage'], axis=1)
y = df['Stage']

# --- 2. Data Preprocessing (Imputation & Encoding) ---

# Separate column types
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()

# Impute Missing Values: Mode for categorical, Median for numerical
for col in categorical_cols:
    X[col].fillna(X[col].mode()[0], inplace=True)

for col in numerical_cols:
    X[col].fillna(X[col].median(), inplace=True)

# One-Hot Encoding
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)

# --- 3. Scaling (CRUCIAL for kNN) ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- 4. Hyperparameter Tuning (Finding the optimal 'k' value) ---
k_range = range(1, 21)
k_scores = []

# Use 10-fold Cross-Validation on the training data to find the best k
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train_scaled, y_train, cv=10, scoring='accuracy')
    k_scores.append(scores.mean())

optimal_k = k_range[np.argmax(k_scores)]
print(f"\nOptimal k found via Cross-Validation: {optimal_k}")

# --- 5. Train Final kNN Model ---
knn_model = KNeighborsClassifier(n_neighbors=optimal_k)
knn_model.fit(X_train_scaled, y_train)

# Predict
y_pred_knn = knn_model.predict(X_test_scaled)

# --- 6. Evaluate Model ---
accuracy_knn = accuracy_score(y_test, y_pred_knn)
conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)

print("\n" + "="*50)
print(f"k-NN CLASSIFICATION PERFORMANCE (Target: Stage, k={optimal_k})")
print("="*50)
print(f"Accuracy: {accuracy_knn:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred_knn))

# --- 7. Plot Confusion Matrix ---
plt.figure(figsize=(8, 7))
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Purples', cbar=False,
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title(f'k-NN Confusion Matrix (Predicting Stage, k={optimal_k})')
plt.ylabel('Actual Stage')
plt.xlabel('Predicted Stage')
plt.tight_layout()
plt.show()

# --- 8. Plot k Value vs. Cross-Validation Accuracy ---
plt.figure(figsize=(10, 6))
plt.plot(k_range, k_scores, marker='o', linestyle='--')
plt.scatter(optimal_k, k_scores[optimal_k - 1], color='red', s=100, label=f'Optimal k={optimal_k}')
plt.title('k-Value vs. Cross-Validation Accuracy') # **Corrected line**
plt.xlabel('Value of k for kNN')
plt.ylabel('Cross-Validated Accuracy')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
# TensorFlow/Keras libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# --- 1. Data Loading ---
# Ensure 'cirrhosis.csv' is uploaded to your Colab environment
file_name = 'cirrhosis.csv'
try:
    df = pd.read_csv(file_name)
    print(f"Successfully loaded {file_name}")
except FileNotFoundError:
    print(f"Error: The file '{file_name}' was not found. Please ensure it is uploaded to Colab.")
    exit()

# Target: Cirrhosis Stage (Multi-class Classification)
df.dropna(subset=['Stage'], inplace=True)
df['Stage'] = df['Stage'].astype(int)

X = df.drop(['ID', 'Status', 'Stage'], axis=1)
y = df['Stage']

# --- 2. Data Preprocessing ---

# Impute Missing Values: Mode for categorical, Median for numerical
categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
numerical_cols = X.select_dtypes(include=np.number).columns.tolist()

for col in categorical_cols:
    X[col].fillna(X[col].mode()[0], inplace=True)
for col in numerical_cols:
    X[col].fillna(X[col].median(), inplace=True)

# One-Hot Encoding
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)

# Scaling (CRUCIAL for Neural Networks)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Target Transformation for Multi-class NN: Shift and One-Hot Encode
# Stages (1, 2, 3, 4) must be shifted to (0, 1, 2, 3) for Keras to_categorical
y_train_shifted = y_train - 1
y_test_shifted = y_test - 1
y_train_one_hot = to_categorical(y_train_shifted, num_classes=4)
y_test_one_hot = to_categorical(y_test_shifted, num_classes=4)


# --- 3. Build and Train the Neural Network Model ---

input_dim = X_train_scaled.shape[1]
output_dim = 4 # Number of stages

model = Sequential()
# First hidden layer
model.add(Dense(64, activation='relu', input_dim=input_dim))
# Second hidden layer
model.add(Dense(32, activation='relu'))
# Output layer with softmax for multi-class prediction
model.add(Dense(output_dim, activation='softmax'))

# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
print("\nTraining Neural Network...")
history = model.fit(X_train_scaled, y_train_one_hot,
                    epochs=50,
                    batch_size=32,
                    validation_data=(X_test_scaled, y_test_one_hot),
                    verbose=0) # Run silently

# --- 4. Evaluate and Predict ---
loss, accuracy_nn = model.evaluate(X_test_scaled, y_test_one_hot, verbose=0)

# Predict classes and convert back to original Stage values (1, 2, 3, 4)
y_pred_proba = model.predict(X_test_scaled, verbose=0)
y_pred_nn_shifted = np.argmax(y_pred_proba, axis=1)
y_pred_nn = y_pred_nn_shifted + 1 # Shift back to 1, 2, 3, 4

conf_matrix_nn = confusion_matrix(y_test, y_pred_nn)

# --- 5. Display Results ---
print("\n" + "="*50)
print(f"NEURAL NETWORK (MLP) CLASSIFICATION PERFORMANCE")
print("="*50)
print(f"Test Accuracy: {accuracy_nn:.4f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred_nn, zero_division=0))


# --- 6. Plot Training History ---
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()


# --- 7. Plot Confusion Matrix ---
plt.figure(figsize=(8, 7))
sns.heatmap(conf_matrix_nn, annot=True, fmt='d', cmap='viridis', cbar=False,
            xticklabels=sorted(y.unique()),
            yticklabels=sorted(y.unique()))
plt.title('Neural Network Confusion Matrix (Predicting Stage)')
plt.ylabel('Actual Stage')
plt.xlabel('Predicted Stage')
plt.tight_layout()
plt.show()